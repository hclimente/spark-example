{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-08T12:41:40.631495Z",
     "start_time": "2017-11-08T12:41:15.230623Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.2.0\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.5.4 (default, Oct 27 2017 11:48:53)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "sparkFile = os.path.join(os.environ[\"SPARK_HOME\"], 'python/pyspark/shell.py')\n",
    "exec(compile(open(sparkFile, \"rb\").read(), sparkFile, 'exec'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The dataset\n",
    "\n",
    "We will work with the NIPS dataset of the [Bag of Words Data Set](https://archive.ics.uci.edu/ml/datasets/bag+of+words). It consists of two files:\n",
    "\n",
    "- `docword.nips.txt` contain shingles of the document. Contains the count of each words in each document. Importantly, stop words were removed, and only words appearing more than 10 times are kept.\n",
    "- `vocab.nips.txt` contains all the used words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-08T12:41:40.662074Z",
     "start_time": "2017-11-08T12:41:40.635221Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "['a2i', 'aaa', 'aaai', 'aapo', 'aat', 'aazhang', 'abandonment', 'abbott', 'abbreviated', 'abcde', 'abe', 'abeles', 'abi', 'abilistic', 'abilities', 'ability', 'abl', 'able', 'ables', 'ablex']\n"
     ]
    }
   ],
   "source": [
    "vocabulary = [ word for word in map(lambda x: x.strip(), open(\"data/vocab.nips.txt\").readlines()) ]\n",
    "\n",
    "# check some stop words\n",
    "print('the' in vocabulary or 'a' in vocabulary or 'to' in vocabulary)\n",
    "\n",
    "# example of shingles\n",
    "print(vocabulary[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal in this practice will be to efficiently find related documents by using local-sensitivity hashing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local-sensitivity hashing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shingling: the documents as sets\n",
    "\n",
    "The document was already shingled, so we will be just pre-process it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-08T12:41:41.373071Z",
     "start_time": "2017-11-08T12:41:40.665637Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "C = sc.textFile (\"data/docword.nips.txt\") \\\n",
    "        .map(lambda line: line.split()) \\\n",
    "        .filter(lambda line: len(line) == 3) \\\n",
    "        .map(lambda y: (y[0], int(y[1]) - 1)) \\\n",
    "        .groupByKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C is an RDD that represents the characteristic matrix of the dataset. A pair (K,V), represent the rows K of column K, which are the only non-zero values. The method we are implementing does not care about how many times an element appears."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minhash: getting document signatures\n",
    "\n",
    "We define the function used to get the document signatures. We will do 1000 random pseudo-permutations, and for each of them, select the first row that is not 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-08T12:41:41.400498Z",
     "start_time": "2017-11-08T12:41:41.375694Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "SIGNATURE_SIZE = 100\n",
    "\n",
    "A = random.sample(range(1,1500), SIGNATURE_SIZE)\n",
    "B = random.sample(range(1,1500), SIGNATURE_SIZE)\n",
    "\n",
    "def min_hash(a, b, sig):\n",
    "    hashes = [((a * x) + b) % len(vocabulary) for x in sig]\n",
    "    return min(hashes)\n",
    "\n",
    "def get_signature(p):\n",
    "    doc,words = p\n",
    "    signature = [ min_hash(a, b, words) for a,b in zip(A,B) ]\n",
    "    return((doc, signature))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSH: getting candidate pairs\n",
    "\n",
    "We will chunk each of the signatures and hash each of the resulting bands. Two bands from two documents falling in the same bucket is unlikely enough to be considered interesting for further study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-08T12:41:41.421777Z",
     "start_time": "2017-11-08T12:41:41.403612Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_BANDS = 10\n",
    "\n",
    "def chunk(l, n):\n",
    "    l = [ x for x in l ]\n",
    "    for i in range(0, len(l), int(len(l)/n)):\n",
    "        yield frozenset(l[i:i + n])\n",
    "        \n",
    "def hash_bands(p):\n",
    "    doc,sig = p   \n",
    "    bands = [ ((i, hash(b)), [doc]) for i,b in enumerate(chunk(sig, NUM_BANDS)) ]\n",
    "    return bands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting it altogether..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-08T12:42:13.554380Z",
     "start_time": "2017-11-08T12:41:41.425705Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "candidates = C \\\n",
    "    .map(get_signature) \\\n",
    "    .flatMap(hash_bands) \\\n",
    "    .reduceByKey(lambda a, b: a + b) \\\n",
    "    .filter(lambda v: len(v[1]) > 1) \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... we get ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-08T12:42:13.575739Z",
     "start_time": "2017-11-08T12:42:13.557114Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((6, -8630840649184911538), ['820', '1497'])]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... a single candidate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Difficulties encountered\n",
    "\n",
    "- PySpark is painful to debug; possibly Spark is better in that regard, as you have access to the full stack natively.\n",
    "    - Errors produce many uninformative lines (full stack, Java + Python). Additionally, because of the way functions are `pickled`, it might not be clear where the error is.\n",
    "    - Hard to attach a PDB. My best solution: download the notebook as a Python script, replacing\n",
    "    ```\n",
    "    import os\n",
    "    sparkFile = os.path.join(os.environ[\"SPARK_HOME\"], 'python/pyspark/shell.py')\n",
    "    exec(compile(open(sparkFile, \"rb\").read(), sparkFile, 'exec'))\n",
    "    ```\n",
    "    by\n",
    "    ```\n",
    "    from pyspark import SparkContext\n",
    "    sc = SparkContext(\"local\", \"App Name\")\n",
    "    ```\n",
    "    and run with `python -m pdb` to debug the demon. I didnÂ´t need to debug the workers; God forbid you ever have to debug on a cluster environment.\n",
    "    - Difficult to access intermediate variables.\n",
    "- Running PySpark/Spark in proxied networks might not be straightforward.\n",
    "- Lazy evaluation leads to some counter-intuitive results. For example, once a faulty command is run, and it leads to an error, its corrected version will also lead to the same error. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
