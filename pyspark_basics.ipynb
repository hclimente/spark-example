{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-08T17:01:38.540890Z",
     "start_time": "2017-11-08T17:01:14.618586Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.2.0\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.5.4 (default, Oct 27 2017 11:48:53)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "sparkFile = os.path.join(os.environ[\"SPARK_HOME\"], 'python/pyspark/shell.py')\n",
    "exec(compile(open(sparkFile, \"rb\").read(), sparkFile, 'exec'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Hello world\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-08T17:01:38.570701Z",
     "start_time": "2017-11-08T17:01:38.544501Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.235.176.85:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "If PySpart was correctly installed, the variable `sc` (for SparkContent) should be initialized. `sc` is used as the driver that tells Spark how to use the cluster resources. In this case, Spark will run locally.\n",
    "\n",
    "Below there is an example of Ï€ estimation using MC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-08T17:01:40.164276Z",
     "start_time": "2017-11-08T17:01:38.574465Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.15552\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "num_samples = 100000\n",
    "\n",
    "def inside(p):     \n",
    "  x, y = random.random(), random.random()\n",
    "  return x*x + y*y < 1\n",
    "\n",
    "count = sc.parallelize(range(0, num_samples)).filter(inside).count()\n",
    "\n",
    "pi = 4 * count / num_samples\n",
    "print(pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# RDDs\n",
    "\n",
    "Resilient Distributed Datasets (RDDs) are sets of Java or Scala objects representing data and that can be operated in parallel. They have a time (and hence are compile-time safe), they are lazy and they are based on the Scala collections API. They are the building blocks of Spark. However, they might present problems with non-JVM languages, such as Python: inefficiency, difficult-to-read problems, etc.\n",
    "\n",
    "RDDs can be created either from a storage source supported by Hadoop of from an existing iterable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-08T17:01:40.278889Z",
     "start_time": "2017-11-08T17:01:40.169073Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data/readme.txt MapPartitionsRDD[3] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RDD from a text file; its a pointer, doesn't load it into memory\n",
    "distFile = sc.textFile (\"data/readme.txt\")\n",
    "distFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-08T17:01:40.296306Z",
     "start_time": "2017-11-08T17:01:40.282363Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[4] at parallelize at PythonRDD.scala:480"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RDD from iterable\n",
    "data = [1, 2, 3, 4, 5]\n",
    "distData = sc.parallelize(data)\n",
    "distData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD operations\n",
    "\n",
    "We can manipulate RDDs through two types of operations, actions and transformations.\n",
    "\n",
    "- Actions return a value (\"actual Python variable\") to the driver program after running a computation on the dataset. `reduce` is an action.\n",
    "- Transformations create a new dataset (RDD) from the existing one. Transformations are lazy. `map` is a transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-08T17:01:40.486734Z",
     "start_time": "2017-11-08T17:01:40.300396Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lineLengths ->  PythonRDD[5] at RDD at PythonRDD.scala:48\n",
      "totalLength ->  2075\n"
     ]
    }
   ],
   "source": [
    "# calculate the length of each line\n",
    "lineLengths = distFile.map(lambda s: len(s))\n",
    "# lazyness: lineLengths is not yet computed\n",
    "print(\"lineLengths -> \", lineLengths)\n",
    "\n",
    "totalLength = lineLengths.reduce(lambda a, b: a + b)\n",
    "# reduce is an action, therefore it is evaluated\n",
    "print(\"totalLength -> \", totalLength)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "When we make operations on RDD, Spark breaks them up into tasks, each of which is executed by an executor. Before execution, Spark computes the task's closure: the variables and methods that must be visible to the executor to perform the task. Then, this closure is sent to the executor. Functions passed can be lambda expressions, local `defs` or top-level functions in a module. Variables are copied, and each executor sees its own copy. If we want different executors to update the value of a variable, we need `Accumulators`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-08T17:01:40.555805Z",
     "start_time": "2017-11-08T17:01:40.490825Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Title:  Bag of Words Data Set'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get first element\n",
    "distFile. first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-08T17:01:40.605433Z",
     "start_time": "2017-11-08T17:01:40.559614Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Title:  Bag of Words Data Set',\n",
       " '',\n",
       " 'Abstract: This data set contains five text collections in the form of bags-of-words.',\n",
       " '',\n",
       " '-----------------------------------------------------\\t']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get first 5 elements\n",
    "distFile. take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-08T17:01:40.652609Z",
     "start_time": "2017-11-08T17:01:40.609465Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Title:  Bag of Words Data Set', '', 'Abstract: This data set contains five text collections in the form of bags-of-words.', '', '-----------------------------------------------------\\t', '', 'Data Set Characteristics: Text', 'Number of Instances: 8000000', 'Area: N/A', 'Attribute Characteristics: Integer', 'Number of Attributes: 100000', 'Date Donated: 2008-03-12', 'Associated Tasks: Clustering', 'Missing Values? N/A', '', '-----------------------------------------------------\\t\\t', '', 'Source:', '', 'David Newman', \"newman '@' uci.edu\", 'University of California, Irvine', '', '-----------------------------------------------------\\t', '', 'Data Set Information:', '', 'For each text collection, D is the number of documents, W is the', 'number of words in the vocabulary, and N is the total number of words', 'in the collection (below, NNZ is the number of nonzero counts in the', 'bag-of-words). After tokenization and removal of stopwords, the', 'vocabulary of unique words was truncated by only keeping words that', 'occurred more than ten times. Individual document names (i.e. a', 'identifier for each docID) are not provided for copyright reasons.', '', 'These data sets have no class labels, and for copyright reasons no', 'filenames or other document-level metadata.  These data sets are ideal', 'for clustering and topic modeling experiments.', '', 'For each text collection we provide docword.*.txt (the bag of words', 'file in sparse format) and vocab.*.txt (the vocab file).', '', 'Enron Emails:', 'orig source: www.cs.cmu.edu/~enron', 'D=39861', 'W=28102', 'N=6,400,000 (approx)', '', 'NIPS full papers:', 'orig source: books.nips.cc', 'D=1500', 'W=12419', 'N=1,900,000 (approx)', '', 'KOS blog entries:', 'orig source: dailykos.com', 'D=3430', 'W=6906', 'N=467714', '', 'NYTimes news articles:', 'orig source: ldc.upenn.edu', 'D=300000', 'W=102660', 'N=100,000,000 (approx)', '', 'PubMed abstracts:', 'orig source: www.pubmed.gov', 'D=8200000', 'W=141043', 'N=730,000,000 (approx)', '', '', '-----------------------------------------------------\\t', '', 'Attribute Information:', '', 'The format of the docword.*.txt file is 3 header lines, followed by', 'NNZ triples:', '---', 'D', 'W', 'NNZ', 'docID wordID count', 'docID wordID count', 'docID wordID count', 'docID wordID count', '...', 'docID wordID count', 'docID wordID count', 'docID wordID count', '---', '', 'The format of the vocab.*.txt file is line contains wordID=n.', '']\n"
     ]
    }
   ],
   "source": [
    "# get all the elements\n",
    "print(distFile. collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-08T17:01:40.792745Z",
     "start_time": "2017-11-08T17:01:40.657391Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', 'docID wordID count', 'N=730,000,000 (approx)']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample 3 elements without replacement\n",
    "distFile. takeSample (False, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-08T17:01:40.848218Z",
     "start_time": "2017-11-08T17:01:40.796448Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count number of elements\n",
    "distFile. count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-08T17:01:40.917941Z",
     "start_time": "2017-11-08T17:01:40.852054Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2075"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# aggregate the elements of the dataset\n",
    "# function takes two args and returns one\n",
    "# for correct parallelization, it should be associative and commutative\n",
    "distFile .map(lambda s: len(s)). reduce(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-08T17:01:40.968685Z",
     "start_time": "2017-11-08T17:01:40.921930Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Title:', 'Bag', 'of', 'Words', 'Data', 'Set'],\n",
       " [],\n",
       " ['Abstract:',\n",
       "  'This',\n",
       "  'data',\n",
       "  'set',\n",
       "  'contains',\n",
       "  'five',\n",
       "  'text',\n",
       "  'collections',\n",
       "  'in',\n",
       "  'the',\n",
       "  'form',\n",
       "  'of',\n",
       "  'bags-of-words.']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply a function to every line\n",
    "# return an iterable of iterables with the results\n",
    "distFile. map(lambda line: line.split()). take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-08T17:01:41.024198Z",
     "start_time": "2017-11-08T17:01:40.972521Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Title:',\n",
       " 'Bag',\n",
       " 'of',\n",
       " 'Words',\n",
       " 'Data',\n",
       " 'Set',\n",
       " 'Abstract:',\n",
       " 'This',\n",
       " 'data',\n",
       " 'set']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply a function to every line\n",
    "# return an iterable with the results\n",
    "distFile. flatMap(lambda line: line.split()). take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-08T17:01:41.109524Z",
     "start_time": "2017-11-08T17:01:41.028042Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['orig source: www.cs.cmu.edu/~enron',\n",
       " 'orig source: books.nips.cc',\n",
       " 'orig source: dailykos.com',\n",
       " 'orig source: ldc.upenn.edu',\n",
       " 'orig source: www.pubmed.gov']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reduce the dataset based on a condition\n",
    "distFile. filter(lambda line: 'source' in line). collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations for key-value pairs (K, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-08T17:01:41.125486Z",
     "start_time": "2017-11-08T17:01:41.113412Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = [(1, 5), (2, 4), (1, 3), (2, 5), (3, 1), (1, 2.5)]\n",
    "distData = sc.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-08T17:01:41.585540Z",
     "start_time": "2017-11-08T17:01:41.129801Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, <pyspark.resultiterable.ResultIterable at 0x10820eb38>),\n",
       " (2, <pyspark.resultiterable.ResultIterable at 0x10820e710>),\n",
       " (3, <pyspark.resultiterable.ResultIterable at 0x10820e518>)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# groups pairs (K, V) in pairs (K, Iterable<V>)\n",
    "distData. groupByKey(). collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-08T17:01:41.738597Z",
     "start_time": "2017-11-08T17:01:41.589579Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 10.5), (2, 9), (3, 1)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# returns pairs (K,V) where V is the reduction of all the inputs with key K\n",
    "distData. reduceByKey(lambda a, b: a + b). collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-08T17:01:42.029330Z",
     "start_time": "2017-11-08T17:01:41.742423Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 5), (1, 3), (1, 2.5), (2, 4), (2, 5), (3, 1)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distData. sortByKey(). collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## DataFrames\n",
    "\n",
    "DataFrames were conceived to overcome the efficiency limitations of RDD. At its core they are still RDD, but with an interface that makes interacting with the data easier and faster. The price to pay is the compile-time safety, which is gone in exchange for the flexibility and, therefore, a more prone to error code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataSets\n",
    "\n",
    "DataSets were developed to reconcile the usability of DataFrames with the safety of RDDs. In Spark, a DataSet can be untyped, and hehce equivalent to a DataFrame, or typed. However, in Python, only untyped is available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- [Spark Programming Guide](https://spark.apache.org/docs/2.2.0/rdd-programming-guide.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
